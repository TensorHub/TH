{"id":"training-0001","axis":"Training","subdomain":"Алгоритмы оптимизации","difficulty":1,"type":"mcq","prompt":"Что такое градиентный спуск?","options":["Спуск на лыжах","Итеративный алгоритм, обновляющий параметры в направлении, противоположном градиенту","Способ регуляризации","Функция потерь"],"answer":1,"explanation":"Gradient Descent: θ ← θ - η∇L(θ), где η — learning rate; минимизирует функцию потерь."}
{"id":"training-0002","axis":"Training","subdomain":"Гиперпараметры","difficulty":1,"type":"truefalse","prompt":"Learning rate (скорость обучения) влияет на размер шагов обновления параметров.","answer":true,"explanation":"Learning rate определяет, насколько большие шаги делает оптимизатор; слишком большой — расхождение, слишком малый — медленно."}
{"id":"training-0003","axis":"Training","subdomain":"Алгоритмы оптимизации","difficulty":2,"type":"mcq","prompt":"Какие варианты градиентного спуска?","options":["Только batch gradient descent","Batch, stochastic (SGD) и mini-batch","Только SGD","Нет вариантов"],"answer":1,"explanation":"Batch GD (на всех данных), SGD (на одном примере), mini-batch (на подмножестве) — trade-off между точностью и скоростью."}
{"id":"training-0004","axis":"Training","subdomain":"Оптимизаторы","difficulty":2,"type":"short","prompt":"Как называется оптимизатор, который использует экспоненциально взвешенные скользящие средние градиентов?","answer":"Adam","explanation":"Adam: Adaptive Moment Estimation; комбинирует momentum и RMSprop для адаптивного learning rate."}
{"id":"training-0005","axis":"Training","subdomain":"Гиперпараметры","difficulty":2,"type":"mcq","prompt":"Что такое batch size?","options":["Размер нейрона","Количество примеров в одной итерации обучения","Общее количество примеров","Название оптимизатора"],"answer":1,"explanation":"Batch Size: более крупные батчи → стабильнее, но медленнее; мелкие → шумнее, но быстрее."}
{"id":"training-0006","axis":"Training","subdomain":"Регуляризация","difficulty":3,"type":"mcq","prompt":"Что такое L2-регуляризация (weight decay)?","options":["Удаление весов","Добавление штрафа (penalty) за большие веса в функцию потерь","Удаление слоёв","Изменение activation function"],"answer":1,"explanation":"L2 penalty: λ||w||² добавляется к loss; предотвращает переобучение через сжатие весов."}
{"id":"training-0007","axis":"Training","subdomain":"Регуляризация","difficulty":3,"type":"short","prompt":"Как называется техника, которая случайно обнуляет activations нейронов во время обучения?","answer":"dropout","explanation":"Dropout: случайно отключает нейроны с вероятностью p; действует как ансамбль моделей."}
{"id":"training-0008","axis":"Training","subdomain":"Алгоритмы оптимизации","difficulty":3,"type":"mcq","prompt":"Что такое momentum в оптимизации?","options":["Скорость обучения","Накопление градиентов из предыдущих итераций для ускорения сходимости","Функция потерь","Batch size"],"answer":1,"explanation":"Momentum: v ← βv + ∇L; помогает преодолеть плато и ускоряет сходимость."}
{"id":"training-0009","axis":"Training","subdomain":"Нормализация","difficulty":3,"type":"truefalse","prompt":"Batch Normalization нормализует входы каждого слоя, улучшая стабильность обучения.","answer":true,"explanation":"Batch Norm: нормализует активации по батчу; уменьшает internal covariate shift."}
{"id":"training-0010","axis":"Training","subdomain":"Останов обучения","difficulty":3,"type":"mcq","prompt":"Что такое early stopping?","options":["Остановка в начале обучения","Остановка обучения, когда validation loss перестаёт улучшаться","Удаление слоёв","Уменьшение batch size"],"answer":1,"explanation":"Early Stopping: монитор validation loss; если не улучшается, останавливаем (предотвращает переобучение)."}
{"id":"training-0011","axis":"Training","subdomain":"Инициализация","difficulty":3,"type":"short","prompt":"Как называется метод инициализации весов для ReLU сетей (He initialization)?","answer":"He initialization","explanation":"He init: выборка из N(0, 2/n) для ReLU; учитывает активацию ReLU (~50% нулей)."}
{"id":"training-0012","axis":"Training","subdomain":"Оптимизаторы","difficulty":4,"type":"mcq","prompt":"Что такое AdaGrad?","options":["Градиент для Ad-hoc данных","Оптимизатор с адаптивным learning rate для каждого параметра","Только для регрессии","Метод регуляризации"],"answer":1,"explanation":"AdaGrad: скорость обучения уменьшается пропорционально сумме квадратов прошлых градиентов."}
{"id":"training-0013","axis":"Training","subdomain":"Регуляризация","difficulty":4,"type":"truefalse","prompt":"L1-регуляризация способствует разреженности весов (sparsity) более сильно, чем L2.","answer":true,"explanation":"L1 penalty: λ|w| (абсолютное значение) толкает малые веса к нулю → sparsity."}
{"id":"training-0014","axis":"Training","subdomain":"Нормализация","difficulty":4,"type":"mcq","prompt":"Что такое Layer Normalization?","options":["Нормализация по примерам в батче","Нормализация по признакам внутри примера (независимо от батча)","Нормализация по всему датасету","Нормализация случайная"],"answer":1,"explanation":"LayerNorm: нормализует признаки для каждого примера; стабильнее BatchNorm на малых батчах."}
{"id":"training-0015","axis":"Training","subdomain":"Гиперпараметры","difficulty":4,"type":"short","prompt":"Как называется процесс подбора лучших гиперпараметров для модели?","answer":"hyperparameter tuning","explanation":"Hyperparameter tuning: поиск оптимальных значений (grid search, random search, Bayesian optimization)."}
{"id":"training-0016","axis":"Training","subdomain":"Останов обучения","difficulty":4,"type":"mcq","prompt":"Что такое learning rate schedule?","options":["Расписание занятий","Уменьшение learning rate во время обучения по определённому плану","Увеличение learning rate","Случайное изменение lr"],"answer":1,"explanation":"Learning Rate Schedule: step decay, exponential decay, cosine annealing; улучшает сходимость."}
{"id":"training-0017","axis":"Training","subdomain":"Алгоритмы оптимизации","difficulty":5,"type":"mcq","prompt":"Что такое RMSprop?","options":["Удаление мусора","Оптимизатор с адаптивным learning rate, использующий экспоненциально взвешенное среднее квадратов градиентов","Метод регуляризации","Batch size"],"answer":1,"explanation":"RMSprop: v ← βv + (1-β)g²; делит градиент на √v для адаптивной скорости."}
{"id":"training-0018","axis":"Training","subdomain":"Регуляризация","difficulty":5,"type":"truefalse","prompt":"Mixup — это техника, которая создаёт синтетические примеры через линейную интерполяцию между примерами.","answer":true,"explanation":"Mixup: x̃ = λx₁ + (1-λ)x₂, ỹ = λy₁ + (1-λ)y₂; улучшает обобщение и robustness."}
{"id":"training-0019","axis":"Training","subdomain":"Специальные техники","difficulty":5,"type":"short","prompt":"Как называется процесс предварительного обучения на большом датасете перед fine-tuning?","answer":"pretraining","explanation":"Pretraining: обучение на большом датасете (ImageNet, Wikipedia) для инициализации весов."}
{"id":"training-0020","axis":"Training","subdomain":"Оптимизаторы","difficulty":5,"type":"mcq","prompt":"Что такое LAMB (Layer-wise Adaptive Moments optimizer for Batch training)?","options":["Ягнёнок","Оптимизатор для больших батчей с адаптивным масштабированием на слой","Только для RNN","Функция потерь"],"answer":1,"explanation":"LAMB: адаптирует learning rate для каждого слоя в зависимости от нормы градиента → лучше на больших батчах."}
